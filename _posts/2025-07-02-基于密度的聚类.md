**基于密度的聚类**是一类无监督聚类方法，核心思想是：将数据空间中密度较高的区域划分为同一类，密度较低的区域视为噪声或边界。常见代表算法有DBSCAN和HDBSCAN。

## Why density-based clustering?

Let’s start with a sample data set.

![DBSCAN]({{ "/assets/images/1_6Plad8ULc5VoOcML5wJjdg.png" | relative_url }})

If you visually try to identify the clusters, you might identify 6 “intuitive” clusters.

Even when provided with the correct number of clusters, K-means clearly gives bad results. Some of the clusters we identified above are separated into two or more clusters. HDBSCAN, on the other hand, gives us the expected clusters.

![DBSCAN]({{ "/assets/images/1__ttZpXcDUmaNmQYwyT2qkQ.png" | relative_url }})

Unlike K-means, density-based methods work well even when the data isn’t clean and the clusters are weirdly shaped. 


**DBSCAN和KMeans的区别：**

区别：

| 方面         | KMeans机制                                      | DBSCAN机制                                      |
|--------------|------------------------------------------------|------------------------------------------------|
| 聚类思想     | 基于距离的中心点分配，最小化簇内平方误差         | 基于密度，寻找密度足够高的区域形成簇             |
| 聚类形状     | 适合球状、均匀分布的簇                          | 可识别任意形状的簇                              |
| 簇数量       | 需预先指定K值（簇数）                           | 不需要指定簇数，自动发现                        |
| 噪声处理     | 不具备噪声识别能力，所有点都要归类               | 能识别噪声点（离群点）                          |
| 参数         | 主要参数为簇数K                                 | 主要参数为邻域半径ε和最小点数MinPts              |
| 对异常敏感性 | 对异常点敏感，异常点会影响聚类中心               | 对异常点鲁棒，异常点会被标记为噪声               |
| 适用场景     | 适合簇大小相近、密度均匀的数据                   | 适合簇密度不均、形状复杂、含噪声的数据           |

## HDBSCAN与DBSCAN的比较   
虽然两者都是基于密度的聚类算法，但它们在处理不同密度簇和噪声数据方面有显著差异：

| 对比项         | DBSCAN                                                         | HDBSCAN                                                         |
|----------------|---------------------------------------------------------------|-----------------------------------------------------------------|
| 参数选择       | 需要预选指定固定的密度阈值α和最小样本数k                      | 不需要固定的密度阈值，通过层次化方法自适应不同密度簇            |
| 处理不同密度簇 | 在处理不同密度簇时表现不佳，因为固定的α可能无法适应所有簇的密度 | 通过构建层次树和稳定性评分，能够有效识别不同密度的簇            |
| 噪声识别       | 只能基于固定阈值识别噪声点                                     | 利用稳定性评分和层次化信息，能够更准确地识别噪声点              |
| 输出结果       | 直接输出一个扁平的聚类结果                                     | 除了扁平聚类结果外，还提供层次树结构和每个簇的稳定性评分        |

## DBSCAN

核心点和簇拓展的办法。

直接密度可达（Directly Density-Reachable）的**定义**：
- 点q从点p直接密度可达，当且仅当：
  - p是核心点（其ε-邻域内至少包含MinPts个点）
  - q位于p的ε-邻域内（即d(p,q) ≤ ε）

直接密度可达的**特点**：
- 单向关系：q从p直接密度可达 ≠ p从q直接密度可达（除非q也是核心点）
- 一步直达：不需要中间点
- 示例：如果p是核心点，q在p的ε邻域内，则q从p直接密度可达

间接密度可达（Indirectly Density-Reachable）的**定义**：
- 点q从点p间接密度可达，意味着：
  - 存在一系列点p₁,p₂,...,pₙ（p₁=p，pₙ=q）
  - 每个pᵢ₊₁从pᵢ直接密度可达（i=1,2,...,n-1）
  - 且n ≥ 3（至少经过一个中间点）

间接密度可达的**特点**：
- 多跳连接：必须通过至少一个中间核心点传递
- 仍然是单向关系
- 示例：p→core₁→core₂→q（p到q通过两个核心点连接）

密度可达（Density-Reachable）的**定义**：
- 点q从点p密度可达，意味着：
  - 存在一系列点p₁,p₂,...,pₙ（p₁=p，pₙ=q）
  - 每个pᵢ₊₁从pᵢ直接密度可达
  - 链条长度n ≥ 2（允许直接或间接连接）

密度可达的**特点**：
- 包含两种情况：
  - 直接密度可达（当n=2时）
  - 间接密度可达（当n≥3时）
- 是前两个概念的超集
- 示例：
  - p→q（直接，n=2）
  - p→core₁→core₂→q（间接，n=4）

Let’s say we have a data set of points like the following. Our goal is to cluster these points into groups that are densely packed together.

![DBSCAN]({{ "/assets/images/2025-07-02/1_dbscan.jpg" | relative_url }})

 First, we count the number of points close to each point. For example, if we start with the yellow point, we draw a circle around it. The radius $\varepsilon$ (邻域半径) of the circle is the first parameter that we have to determine when using DBSCAN.       
After drawing the circle, we count the overlaps. For example, for our yellow point, there are five close points.

![DBSCAN]({{ "/assets/images/2025-07-02/2_dbscan.jpg" | relative_url }})

Likewise, we count the number of close points for all remaining points.

![DBSCAN]({{ "/assets/images/2025-07-02/3_dbscan.jpg" | relative_url }})

Next, we will determine another parameter, the minimum number of points $\text{MinPts}$. Each point is considered a core point if they are close to at least m other points. For example, if we take m as three, then, purple points are considered as **core points** (核心点), but the yellow one is not because it doesn’t have any close points around it. Identifying four purple core points and one yellow outlier.

$$N\{O(\varepsilon)\} \geq \text{MinPts}$$

![DBSCAN]({{ "/assets/images/2025-07-02/4_dbscan.jpg" | relative_url }})

All core data points in purple.

![DBSCAN]({{ "/assets/images/2025-07-02/5_dbscan.jpg" | relative_url }})

Then, we randomly select a core point and assign it as the first point in our first cluster (随机选择一个未访问的核心点作为种子). Other points that are close to this point are also assigned to the same cluster (i.e., within the circle of the selected point).

![DBSCAN]({{ "/assets/images/2025-07-02/6_dbscan.jpg" | relative_url }})

Then, we extend it to the other points that are close (簇扩展).

![DBSCAN]({{ "/assets/images/2025-07-02/7_dbscan.jpg" | relative_url }})

We stop when we can’t assign more core points to the first cluster. Some core points couldn’t be appointed even though they were close to the first cluster. We draw the circles of these points and see if the first cluster is close to the core points. If there is an overlap, we put them in the first cluster.        
However, we can’t assign any non-core points. They are considered outliers.

![DBSCAN]({{ "/assets/images/2025-07-02/8_dbscan.jpg" | relative_url }})

Now, we still have core points that aren’t assigned. We randomly select another point from them and start over.

![DBSCAN]({{ "/assets/images/2025-07-02/9_dbscan.jpg" | relative_url }})

所有既不是核心点，也不在任何核心点的ε-邻域内的点被标记为噪声/离群点。这些点不会被归入任何簇。

## HDBSCAN

### HDBSCAN简介 

HDBSCAN（Hierarchical Density-Based Spatial Clustering of Applications with Noise）is a significant step beyond earlier density-based clustering algorithms like DBSCAN, primarily due to its ability to identify clusters of varying densities and its hierarchical approach to clustering. This approach allows the algorithm to determine the optimal number of clusters, a much-coveted feature not often seen in other clustering algorithms. Let’s dive into the steps that HDBSCAN takes to form the clusters. 

- **主要特点**：
  - 不需要固定的密度阈值，能自适应不同密度的数据簇。
  - 通过构建密度可达的层次树结构，自动选择最优聚类结果。
  - 能更好地处理高维、复杂、密度变化大的数据。
  - 能有效识别噪声点。

- **适用场景**：高维、复杂结构、簇密度不均的数据聚类任务。

How does HDBSCAN do this? At a high level, we can simplify the process of density-based clustering into these steps:

1. Estimate the densities 
2. Pick regions of high density
3. Combine points in these selected regions


### Transforming the Space
The first step in HDBSCAN is to transform the feature space using a metric known as the “mutual reachability distance.” Unlike simple Euclidean or Manhattan distances, this metric incorporates density into the distance calculation between points. For any two data points 

a and b, the mutual reachability distance is defined as:

$$\text{MRD}(a, b) = \text{max} (coreDistance_k (a ), coreDistance_k (b ), d(a, b))$$

In the equation above, d(a,b) is just the distance between point a and b, while coreDistance is defined as the distance from a point to its kth nearest neighbor. The value of k is usually left up to the user to choose, and effectively incorporates local density into the calculation. 

![DBSCAN]({{ "/assets/images/1_XI359LqPheRAR4me3-jFtg.png" | relative_url }})

Transforming the space with mutual reachability distances helps to emphasize the density-based relationships between points, making the algorithm sensitive to local density variations. This step sets the stage for the rest of the HDBSCAN algorithm.

### Pick regions of high density

#### Constructing the Minimum Spanning Tree (MST)

Once the mutual reachability distances between all pairs of points are calculated, the next step is to construct a Minimum Spanning Tree (MST) based on these distances. An MST is a tree that connects all the points in such a way that the sum of the edge weights (in this case, mutual reachability distances) is minimized.

Algorithms like Prim’s can be used to efficiently construct the MST. The edges of the MST represent the relationships based on mutual reachability, connecting points in a way that highlights the underlying density structure.

#### Building the Hierarchy
HDBSCAN constructs a hierarchical tree from the MST, a structure known as the “cluster hierarchy.” In this process, each leaf node of the tree corresponds to a single data point, and as we traverse upwards, nodes merge based on edge weight—i.e., mutual reachability distance—in the MST.

Starting from leaf nodes, the algorithm iteratively merges nodes or clusters together based on the smallest edge weight, moving upwards to form a complete binary tree known as the dendrogram.

### Condensing the Tree

After building the cluster hierarchy, the next step is to condense this tree. This process relies on the notion of “stability” of clusters over varying density thresholds. Clusters that are less stable over these thresholds are pruned. The result is a “condensed cluster tree,” which is a simplified version of the initial hierarchical tree. It retains only those clusters that have shown enough stability over the various density levels.

### Extracting the Final Clusters

The last step in the HDBSCAN algorithm involves extracting the final clusters from the condensed tree. Every data point is then labeled according to the cluster it belongs to in this condensed tree. Notably, some points may not belong to any cluster, classified as “noise,” and typically assigned a label of -1. The flexibility in the algorithm allows it to identify clusters of different shapes and densities, providing a nuanced categorization compared to many other clustering algorithms.

![DBSCAN]({{ "/assets/images/1_rWmux9oRT-5cPtfK6i3TNA.jpg" | relative_url }})
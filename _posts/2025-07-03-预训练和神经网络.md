Perceptron Learning (感知器学习), BERT，预训练。

## Perceptron Learning, The Simplest Artificial neural network

The famous Perceptron Learning Algorithm was originally proposed by Frank Rosenblatt in 1943, later refined and carefully analyzed by Minsky and Papert in 1969. This is a follow-up post of my previous posts on the McCulloch-Pitts neuron model and the Perceptron model.

The perceptron model is a more general computational model than McCulloch-Pitts neuron. It takes an input, aggregates it (weighted sum) and returns 1 only if the aggregated sum is more than some threshold else returns 0 (大于某个阈值返回1，否则0). Rewriting the threshold as shown above and making it a constant input with a variable weight, we would end up with something like the following:

![Perceptron]({{ "assets/images/2025-07-03/1_Fyapb-JRFJ-VtnLYLLXCwg.jpg" | relative_url }})

The perceptron model is a more general computational model than McCulloch-Pitts neuron. It takes an input, aggregates it (weighted sum) and returns 1 only if the aggregated sum is more than some threshold else returns 0. Rewriting the threshold as shown above and making it a constant input with a variable weight ($i = 0$), we would end up with something like the following:

![Perceptron]({{ "assets/images/2025-07-03/1_gKFs7YU44vJFiS2rF3-bpg.jpg" | relative_url }})

A single perceptron can only be used to implement linearly separable functions (binary classification). It takes both real and boolean inputs and associates a set of weights to them, along with a bias (the threshold thing I mentioned above). We learn the weights, we get the function. Let's use a perceptron to learn an OR function.

What’s going on above is that we defined a few conditions (the weighted sum has to be more than or equal to 0 when the output is 1) based on the OR function output for various sets of inputs, we solved for weights based on those conditions and we got one of the possible lines that perfectly separates positive inputs from those of negative.

![Perceptron]({{ "assets/images/2025-07-03/1_C5LeL8JDfoGbkUg0cu1M-w.jpg" | relative_url }})

Our goal is to find the w vector that can perfectly classify positive inputs and negative inputs in our data. I will get straight to the algorithm. 

We initialize w with some random vector. We then iterate over all the examples in the data, ($P \cup N$) both positive and negative examples. Now if an input x belongs to P, ideally what should the dot product $w \cdot x$ be? I’d say $w \cdot x \geq 0$ because that’s the only thing what our perceptron wants at the end of the day so let's give it that. And if x belongs to N, the dot product MUST be less than 0. 

Why Would The Specified Update Rule Work? We have already established that when x belongs to P, we want $w \cdot x \geq 0$, basic perceptron rule. What we also mean by that is that when x belongs to P, the angle between w and x should be less than 90 because the cosine of the angle is proportional to the dot product. So whatever the w vector may be, as long as it makes an angle less than 90 degrees with the positive example data vectors (x E P) and an angle more than 90 degrees with the negative example data vectors (x E N), we are cool. So ideally, it should look something like this:

![Perceptron]({{ "assets/images/2025-07-03/1_D09EzbR-sGbX-qv2jcEPhw.jpg" | relative_url }})

So we now strongly believe that the angle between w and x should be less than 90 when x belongs to P class and the angle between them should be more than 90 when x belongs to N class. Pause and convince yourself that the above statements are true and you indeed believe them. Here’s why the update works: when we are adding x to w, which we do when x belongs to P and w.x < 0 (Case 1, left one), we are essentially increasing the cos(alpha) value, which means, we are decreasing the alpha value, the angle between w and x, which is what we desire. And the similar intuition works for the case when x belongs to N and w.x ≥ 0 (Case 2, right one).

![Perceptron]({{ "assets/images/2025-07-03/1_Ny1n6xH8g2JR2XVhcGRVvA.jpg" | relative_url }})

![Perceptron]({{ "assets/images/2025-07-03/1_PbJBdf-WxR0Dd0xHvEoh4A.jpg" | relative_url }})
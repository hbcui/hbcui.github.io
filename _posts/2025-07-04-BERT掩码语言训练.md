掩码语言模型和BERT做掩码语言模型预训练。

## 掩码语言训练

**MLM（Masked Language Modeling，掩码语言模型）** 是一种自监督语言模型训练方法，常用于BERT等模型的预训练阶段。

**原理简述：**
- 在一段文本中，随机选取部分token（词或字）用特殊符号（如 [MASK]）替换。
    - 我想吃[?]
- 模型的任务是根据上下文预测这些被遮盖（masked）的token原本是什么。
    - 目标：预测 [?] = "苹果"

![mlm]({{ "assets/images/2025-07-04/1_r5MXcUkbi9bs9J78Cozhdg.webp" | relative_url }})

## BERT

BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection introduced by Google in 2018, has revolutionized the natural language processing (NLP) domain.

Historically, language models could only read input text sequentially, either left-to-right or right-to-left, but couldn't do both at the same time. BERT is different because it's designed to read in both directions at once. The introduction of transformer models enabled this capability, which is known as bidirectionality. Using bidirectionality, BERT is pretrained on two different but related NLP tasks: masked language modeling (MLM) and next sentence prediction (NSP).
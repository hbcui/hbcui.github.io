服务器bert模型训练。初步结果探索。

## 准备阶段

### 项目代码，数据集

首先查看代码的上级目录，其中有关的内容在李崇的同名目录`./lic`下，据推测其他目录名也来自人名缩写：

```
/mnt/tenant-home_speed/encoder_hlf_yjh/lic/
├── lic
├── jjun
├── hlf
└── yjh
└── ...
```

按照这里的目录名习惯，创建自己的同名目录`./cui-hb`，之后把李崇目录下的内容全部拷贝到自己的目录下。

```bash
cp -r lic/* cui-hb/
```

### 脚本目录

先给 shell 脚本加执行权限：

```bash
chmod +x /mnt/tenant-home_speed/encoder_hlf_yjh/cui-hb/A80030132_Traffic_llm/visualize_sh/lc_run_pretrain_train.sh
```

直接用 bash 或 ./ 执行 shell 脚本。注意脚本运行时的工作目录。

```bash
cd /mnt/tenant-home_speed/encoder_hlf_yjh/cui-hb/A80030132_Traffic_llm && bash visualize_sh/lc_run_pretrain_train.sh
```

## 预训练

`lc_run_pretrain_train.sh`是没有进行patch方法的。今天先尝试运行这个，看看能不能成功。

### 脚本参数配置

脚本开始运行了，但遇到了两个问题：

* 数据处理错误：'float' object has no attribute 'split' - 数据格式有问题
* GPU内存不足：CUDA out of memory - 显存不够

解决方案：

* 减少batch size：修改脚本中的 --batch_size_list 4096 为更小的值，比如 1024 或 512。
* 减少micro batch size：修改 --micro_batch_size_list 128 为更小的值，比如 32 或 64。

服务器下查看gpu。

```
nvitop
```

![mlm]({{ "assets/images/2025-07-08/2025-07-08_17-13.png" | relative_url }})

## 输出

首先关注输出的目录结构。

```
/mnt/tenant-home_speed/encoder_hlf_yjh/cui-hb/exp-274
├── checkpoints
├── logs
├── tensoroard_logs
└── saved_params.py
```

**BERT类的预训练模型，当前运行的是MLM（Masked Language Model）任务输出内容如下：**

1. **训练阶段输出：**
   - 每个step会输出：
     - MLM-Loss（掩码语言模型损失）
     - MLM-Acc（掩码token预测准确率）
     - MLM-Top10（Top10准确率）
     - MLM-All-Tokens（全token准确率）
     - MLM-F1（F1分数）

2. **最终产物：**
   - **模型权重文件**（exp-274/checkpoints/，如best.pth等）
   - **TensorBoard日志**（exp-274/tensorboard_logs/，可用于可视化loss/acc曲线）
   - **训练参数配置**（exp-274/saved_params.py）
   - **训练日志**（exp-274/logs/）

3. **模型本身的输出（推理时）：**
   - 输入一个被mask的流量序列，输出每个token的预测分布（概率），可还原被mask的内容。
   - 也可用于下游任务（如分类、NER等），输出为特征向量或类别概率。

### 日志

因为我在shell脚本设置的资源比较小， 所以从上午10：20左右到下午1点多完成（需要大概3个小时），对应服务器时间02:22 -> 05:13。

```
[INFO] [2025-07-08 02:22:25,050] [log_support:log_message]: [line:99] [INFO] Reconstructing vocab dict...
[INFO] [2025-07-08 02:22:25,217] [log_support:log_message]: [line:99] [INFO] Building model...
[INFO] [2025-07-08 02:22:25,217] [log_support:log_message]: [line:99] [INFO] Initializing model architecture...
[INFO] [2025-07-08 02:22:29,186] [log_support:log_message]: [line:99] [INFO] Moving model to GPU...
[INFO] [2025-07-08 02:22:29,772] [log_support:log_message]: [line:99] [INFO] Wrapping model with DDP...
[INFO] [2025-07-08 02:22:30,031] [log_support:log_message]: [line:99] ┌════════════════════════════════════════════════════════════┐
│                      STAGE-1 TRAINING                      │
├════════════════════════════════════════════════════════════┤
│                      Stage Parameters                      │
│────────────────────────────────────────────────────────────│
│ Sequence Length    │      256                              │
│ World Size         │        1                              │
│ Batch Size         │       64                              │
│ Micro Batch        │        4                              │
│ Num Epochs         │      3.0                              │
└════════════════════════════════════════════════════════════┘

[INFO] [2025-07-08 02:22:30,033] [log_support:log_message]: [line:99] [INFO] Loading stage-training dataset from: ".../packet_all256/test_random_50_5000.csv"...
[INFO] [2025-07-08 02:22:33,998] [log_support:log_message]: [line:99] [INFO] Loading stage-testing dataset from: ".../packet_all256/test_random_50_5000.csv"...
[INFO] [2025-07-08 02:22:38,211] [log_support:log_message]: [line:99] ┌────────────────────────────────────────────────────────────┐
│                   TRAINING CONFIGURATION                   │
├────────────────────────────────────────────────────────────┤
│                        Environment                         │
│────────────────────────────────────────────────────────────│
│ World Size           │        1                            │
│ Sequence Mode        │     flow                            │
│ Keep Tail            │    False                            │
├────────────────────────────────────────────────────────────┤
│                       Training steps                       │
│────────────────────────────────────────────────────────────│
│ Max Steps            │      848                            │
│ Warmup Steps         │       84                            │
│ Annealing Steps      │      763                            │
│ Gradient Accum Steps │       16                            │
├────────────────────────────────────────────────────────────┤
│                       Train Dataset                        │
│────────────────────────────────────────────────────────────│
│ Total Samples        │   18,092                            │
│ Samples per GPU      │   18,092                            │
│ Mini Batch Size      │        4                            │
│ Batches per GPU      │    4,523                            │
│ Global Batch Size    │       64                            │
│ Dropped Samples      │        0                            │
└────────────────────────────────────────────────────────────┘
[INFO] [2025-07-08 02:22:51,797] [log_support:log_message]: [line:99] >>> Train     Stage: 1   Step: 1/848   MLM-Loss: 11.0998   MLM-Acc: 0.0000   MLM-Top10: 0.0001   MLM-All-Tokens: 0.0000   MLM-F1: 0.0000
[INFO] [2025-07-08 02:22:52,261] [log_support:log_message]: [line:99] >>> Test      Stage: 1   Step: 1/848   MLM-Loss: 11.5723   MLM-Acc: 0.0000   MLM-Top10: 0.0000   MLM-All-Tokens: 0.0000   MLM-F1: 0.0000
[INFO] [2025-07-08 02:22:54,032] [log_support:log_message]: [line:99] Best model updated: /mnt/tenant-home_speed/encoder_hlf_yjh/cui-hb/exp-274/checkpoints/best.pth
................................................................
................................................................
................................................................
................................................................
[INFO] [2025-07-08 05:13:00,556] [log_support:log_message]: [line:99] >>> Test      Stage: 1   Step: 845/848   MLM-Loss: 1.1533   MLM-Acc: 0.7983   MLM-Top10: 0.8361   MLM-All-Tokens: 0.9504   MLM-F1: 0.4014
[INFO] [2025-07-08 05:13:12,275] [log_support:log_message]: [line:99] >>> Train     Stage: 1   Step: 846/848   MLM-Loss: 1.8456   MLM-Acc: 0.6990   MLM-Top10: 0.7309   MLM-All-Tokens: 0.9269   MLM-F1: 0.2430
[INFO] [2025-07-08 05:13:12,609] [log_support:log_message]: [line:99] >>> Test      Stage: 1   Step: 846/848   MLM-Loss: 0.9694   MLM-Acc: 0.8476   MLM-Top10: 0.8873   MLM-All-Tokens: 0.9629   MLM-F1: 0.5557
```

### 打印评价指标

截取日志最末行。

```
Test      Stage: 1   Step: 846/848   MLM-Loss: 0.9694   MLM-Acc: 0.8476   MLM-Top10: 0.8873   MLM-All-Tokens: 0.9629   MLM-F1: 0.5557
                  ||
                  ||
                  ||
                  \/
最终评估结果（Step 846/848）：
MLM-Loss: 0.9694 (损失值)
MLM-Acc: 0.8476 (准确率 84.76%)
MLM-Top10: 0.8873 (Top10准确率 88.73%)
MLM-All-Tokens: 0.9629 (所有token准确率 96.29%)
MLM-F1: 0.5557 (F1分数 55.57%)
```

### TensorBoard_logs

这里我使用端口6007。

```bash
http://root@ctmt2505191020412if.ssh.zte.com.cn:6007/
```

本地终端运行SSH隧道命令, 这条命令会把你本地的6007端口映射到服务器的6007端口。

```bash
ssh -L 6007:localhost:6007 root@ctmt2505191020412if.ssh.zte.com.cn
```

最后可以在本地浏览器访问： http://localhost:6007

#### Learning rate

![mlm]({{ "assets/images/2025-07-08/Learning rate.svg" | relative_url }})

#### Sequence length

![mlm]({{ "assets/images/2025-07-08/Sequence length.svg" | relative_url }})

#### Test accuracy

![mlm]({{ "assets/images/2025-07-08/Test Accuracy.svg" | relative_url }})

![mlm]({{ "assets/images/2025-07-08/Test_Accuracy.png" | relative_url }})

#### Test loss

![mlm]({{ "assets/images/2025-07-08/Test Loss.svg" | relative_url }})

![mlm]({{ "assets/images/2025-07-08/Test_Loss.png" | relative_url }})

#### Train accuracy

![mlm]({{ "assets/images/2025-07-08/Train Accuracy.svg" | relative_url }})

![mlm]({{ "assets/images/2025-07-08/Train_Accuracy.png" | relative_url }})

#### Train loss

![mlm]({{ "assets/images/2025-07-08/Train Loss.svg" | relative_url }})

![mlm]({{ "assets/images/2025-07-08/Train_Loss.png" | relative_url }})
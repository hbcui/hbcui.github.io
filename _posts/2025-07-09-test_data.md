通过nbconvert转化jupyter notebook为这篇文章。

## Simple Data


```python
def create_sample_data():
    """创建示例数据文件"""
    sample_data = [
        {
            'five_tuple': '192.168.31.160:38268_39.136.197.119:443_TCP',
            'app_name': 'cn.emagsoftware.gamehall',
            'app_version': '3.48.1.1',
            'flow_data': '1603 0301 0102 0200 0001 0100 0001 01fc fc03 0303 039b 9bb5 b56a 6af7 f704 040b 0b3d 3da7 a769 691c 1c44 449c 9ca4 a4f6 f6f9 f9aa aa46 463a 3a9d 9dec ec9e 9e2c 2cd6 d671 7174 7440 404f 4f3d 3d76 7665 65e8 e806 0620 202c 2cbc bccc cc1e 1e66 66fe fe7e 7e83 83f7 f7c8 c8b8 b8fc fc27 2707 079e 9ebd bd8f 8fb9 b92d 2d6c 6cbe be51 51ca ca02 02f9 f9fd fda1 a1a4 a401 01b2 b258 58ac ac00 0020 201a 1a1a 1a13 1301 0113 1302 0213 1303 03c0 c02b 2bc0 c02f 2fc0 c02c 2cc0 c030 30cc cca9 a9cc cca8 a8c0 c013 13c0 c014 1400 009c 9c00 009d 9d00 002f 2f00 0035 3501 0100 0001 0193 939a 9a9a 9a00 0000 0000 0000 0000 0011 1100 000f 0f00 0000 000c 0c70 706c 6c75 7573 732e 2e6d 6d69 6967 6775 752e 2e63 636e 6e00 0017 1700 0000 00ff ff01 0100 0001 0100 0000 000a 0a00 000a 0a00 0008 085a 5a5a 5a00 001d 1d00 0017 1700 0018 1800 000b 0b00 0002 0201 0100 0000 0023 2300 0000 0000 0010 1000 000e 0e00 000c 0c02 0268 6832 3208 0868 6874 7474 7470 702f 2f31 312e 2e31 3100 0005 0500 0005 0501 0100 0000 0000 0000 0000 000d 0d00 0012 1200 0010 1004 0403 0308 0804 0404 0401 0105 0503 0308 0805 0505 0501 0108 0806 0606 0601 0100 0012 1200 0000 0000 0033 3300 002b 2b00 0029 295a 5a5a 5a00 0001 0100 0000 001d 1d00 0020 20ff ff9d 9d53 5371 71be be30 3036 3683 83d5 d510 1020 2003 0362 6271 71ab ab53 5341#1603 0303 0300 007a 7a02 0200 0000 0076 7603 0303 033b 3b32 32e3 e363 638d 8d01 013e 3e19 19df df00 00c3 c38c 8c73 7396 96da da28 284b 4b09 0929 29be be42 4231 3195 95d6 d6db db23 23b5 b509 09c2 c234 34b5 b502 0220 202c 2cbc bccc cc1e 1e66 66fe fe7e 7e83 83f7 f7c8 c8b8 b8fc fc27 2707 079e 9ebd bd8f 8fb9 b92d 2d6c 6cbe be51 51ca ca02 02f9 f9fd fda1 a1a4 a401 01b2 b258 58ac ac13 1302 0200 0000 002e 2e00 002b 2b00 0002 0203 0304 0400 0033 3300 0024 2400 001d 1d00 0020 2026 2632 32f6 f6bf bf12 1233 3353 53d5 d576 7677 77ed ed7d 7deb eb86 861b 1bb4 b49b 9bcb cb3d 3d83 836c 6cf8 f8b6 b6d2 d2d2 d213 1370 70bf bf35 3530 309d 9d78 7814 1403 0303 0300 0001 0101 0117 1703 0303 0300 002a 2a82 824c 4c66 66cf cf01 01e2 e206 06ec ecba bab1 b18b 8b8b 8bd7 d7ac ac7b 7bcd cd7d 7dac ac4d 4d5b 5bf3 f3b7 b721 2122 2276 76ff ff56 5647 473c 3cf0 f090 901c 1c4a 4a1c 1c9b 9bdd ddcb cb7a 7af9 f984 8456 5687 8717 1703 0303 0310 102b 2bce ce3f 3fc8 c81c 1c67 673b 3b4e 4e27 2794 94b8 b838 381d 1df9 f9bc bc4e 4e08 0825 259b 9bae aed3 d376 76c4 c4c5 c57a 7a27 2728 28ff ffb2 b26d 6d5c 5ce1 e1ff ffcc ccb1 b197 973e 3e76 7660 608a 8a89 89b3 b3b7 b740 40ea ea48 48fa fa0e 0e5b 5bda da19 198b 8bfd fd75 75a1 a168 6839 39ca ca23 235a 5aef ef24 24bc bc2e 2ee7 e77f 7ff4 f4b4 b41b 1beb eb66 669a 9a10'
        },
        {
            'five_tuple': '112.25.106.104:443_192.168.31.25:40704_TCP',
            'app_name': 'cn.emagsoftware.gamehall',
            'app_version': '3.61.1.1',
            'flow_data': '1603 0301 0102 0200 0001 0100 0001 01fc fc03 0303 0388 88f5 f5ce ce21 2149 49a2 a219 19e0 e0e8 e8d6 d640 405a 5a0f 0f56 564c 4c4d 4db9 b9ff ff25 2536 36b2 b207 0774 7479 79e3 e31f 1f20 203b 3bc3 c356 56ff ffaa aa20 20d9 d90e 0e3b 3b33 3362 6211 1186 86a6 a678 781a 1a94 9465 65f5 f57d 7d94 94d3 d3d0 d08c 8c14 1450 5084 8422 2256 56ae aec1 c146 468d 8d75 7547 4796 96ce cef9 f900 001e 1e13 1301 0113 1302 0213 1303 03c0 c02b 2bc0 c02c 2ccc cca9 a9c0 c02f 2fc0 c030 30cc cca8 a8c0 c013 13c0 c014 1400 009c 9c00 009d 9d00 002f 2f00 0035 3501 0100 0001 0195 9500 0000 0000 0019 1900 0017 1700 0000 0014 1462 6265 6574 7461 6167 6761 616d 6d65 652e 2e6d 6d69 6967 6775 7566 6675 756e 6e2e 2e63 636f 6f6d 6d00 0017 1700 0000 00ff ff01 0100 0001 0100 0000 000a 0a00 0008 0800 0006 0600 001d 1d00 0017 1700 0018 1800 000b 0b00 0002 0201 0100 0000 0023 2300 0000 0000 0010 1000 000e 0e00 000c 0c02 0268 6832 3208 0868 6874 7474 7470 702f 2f31 312e 2e31 3100 0005 0500 0005 0501 0100 0000 0000 0000 0000 000d 0d00 0014 1400 0012 1204 0403 0308 0804 0404 0401 0105 0503 0308 0805 0505 0501 0108 0806 0606 0601 0102 0201 0100 0033 3300 0026 2600 0024 2400 001d 1d00 0020 2065 65a1 a1e7 e7a2 a280 806c 6c42 42c7 c793 936f 6f52 5233 339b 9b7a 7a38 3823 237e 7e0c 0c9c 9cfb fb06 06cc cc6f 6f18#1603 0303 0300 0056 5602 0200 0000 0052 5203 0303 0364 64e0 e0d5 d522 22e0 e0cc cc5e 5e05 0527 27d9 d95b 5b43 4340 40ea ea76 76e5 e5d2 d20d 0dd5 d52c 2cac acc7 c7d2 d278 7817 1799 9966 66fc fce5 e598 9894 941f 1f20 205c 5cea ea51 51b4 b451 5195 95be be5b 5b21 219d 9d8a 8a0d 0da5 a51c 1cdd dd13 132a 2a6f 6f37 37ee eee4 e4bc bcb2 b297 9708 0835 35ea ea66 66e1 e1ac ac0b 0b8b 8b00 009d 9d00 0000 000a 0a00 0000 0000 0000 0000 000b 0b00 0002 0201 0100 0016 1603 0303 030b 0bcd cd0b 0b00 000b 0bc9 c900 000b 0bc6 c600 0006 06a6 a630 3082 8206 06a2 a230 3082 8205 058a 8aa0 a003 0302 0201 0102 0202 0210 1001 01a0 a0f6 f6cc cc77 77bb bb9e 9e2f 2ff7 f7dd ddd9 d986 865d 5d5b 5bad adba ba30 300d 0d06 0609 092a 2a86 8648 4886 86f7 f70d 0d01 0101 010b 0b05 0500 0030 305b 5b31 310b 0b30 3009 0906 0603 0355 5504 0406 0613 1302 0255 5553 5331 3115 1530 3013 1306 0603 0355 5504 040a 0a13 130c 0c44 4469 6967 6769 6943 4365 6572 7274 7420 2049 496e 6e63 6331 3119 1930 3017 1706 0603 0355 5504 040b 0b13 1310 1077 7777 7777 772e 2e64 6469 6967 6769 6963 6365 6572 7274 742e 2e63 636f 6f6d 6d31 311a 1a30 3018 1806 0603 0355 5504 0403 0313 1311 1153 5365 6563 6375 7572 7265 6520 2053 5369 6974 7465 6520 2043 4341 4120 2047 4732 3230 301e 1e17 170d 0d32 3233 3330 3033 3331 3134 3430 3030'
        }
    ]
    
    # 创建 DataFrame 并保存
    df = pd.DataFrame(sample_data)
    df.to_csv('sample_traffic_data.csv', index=False)
    print("示例数据已保存到 sample_traffic_data.csv")
    return 'sample_traffic_data.csv'
```


```python
# 演示数据处理流程
print("=== 流量数据预训练处理演示 ===\n")

# 1. 创建示例数据
data_path = create_sample_data()
print(data_path)
```

    === 流量数据预训练处理演示 ===
    
    示例数据已保存到 sample_traffic_data.csv
    sample_traffic_data.csv


## Tokenizer

SimpleTokenizer 词表大小是 261，原因如下：

* 特殊标记：5个（[PAD], [UNK], [CLS], [SEP], [MASK]）
    * [UNK] 是 unknown token（未知标记）的缩写。当分词器（tokenizer）遇到词表中没有的内容时，会用 [UNK] 代替，表示“未登录词”或“无法识别的token”。在你的 SimpleTokenizer 里，如果输入的十六进制字符串不是 00~ff 或特殊标记，就会被编码为 [UNK] 对应的ID（即1）。
* 十六进制字符串：256个（00 ~ ff，即0~255）

$$16^2 + 5 = 261$$


```python
#!/usr/bin/env python3
"""
流量数据预训练处理示例
基于 run_pretrain.py 的数据处理流程
"""

import os
import pandas as pd
import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
import numpy as np
from typing import List, Dict, Optional, Tuple

# 模拟 tokenizer 类
class SimpleTokenizer:
    """简化的 tokenizer，用于演示"""
    
    def __init__(self):
        # 创建词汇表：十六进制字符 + 特殊标记
        self.vocab = {}
        self.id_to_token = {}
        
        # 添加特殊标记
        special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']
        for i, token in enumerate(special_tokens):
            self.vocab[token] = i
            self.id_to_token[i] = token
        
        # 添加十六进制字符 (00-ff)
        for i in range(256):
            hex_str = f"{i:02x}"
            self.vocab[hex_str] = i + len(special_tokens)
            self.id_to_token[i + len(special_tokens)] = hex_str
        
        self.pad_token_id = self.vocab['[PAD]']
        self.unk_token_id = self.vocab['[UNK]']
        self.cls_token_id = self.vocab['[CLS]']
        self.sep_token_id = self.vocab['[SEP]']
        self.mask_token_id = self.vocab['[MASK]']
    
    def encode(self, hex_string: str) -> List[int]:
        """将十六进制字符串编码为 token IDs"""
        # 分割十六进制字符串
        hex_parts = hex_string.split()
        tokens = []
        
        for part in hex_parts:
            if part in self.vocab:
                tokens.append(self.vocab[part])
            else:
                tokens.append(self.unk_token_id)
        
        return tokens
    
    def decode(self, token_ids: List[int]) -> str:
        """将 token IDs 解码为十六进制字符串"""
        return ' '.join([self.id_to_token.get(tid, '[UNK]') for tid in token_ids])
    
    def __len__(self):
        return len(self.vocab)

class TrafficDataset:
    """流量数据集类，模拟 SignalDataset 的核心功能"""
    
    def __init__(self, data_path: str, tokenizer, maxlen: int = 512, sequence_mode: str = 'packet'):
        self.tokenizer = tokenizer
        self.maxlen = maxlen
        self.sequence_mode = sequence_mode
        
        # 读取数据
        self.data = pd.read_csv(data_path)
        print(f"加载数据: {len(self.data)} 条记录")
        
        # 处理数据
        self.examples, self.labels, self.procedure_mapping = self._process_data()
        
    def _process_data(self):
        """处理流量数据"""
        examples = []
        labels = []
        procedure_names = set()
        
        for idx, row in self.data.iterrows():
            five_tuple = row['five_tuple']
            app_name = row['app_name']
            flow_data = row['flow_data']
            
            # 收集应用名称
            procedure_names.add(app_name)
            
            # 按 # 分割流量数据得到多个包
            packets = [
                p.strip() for p in flow_data.split('#')
                if p.strip() != '0000' and p.strip() != ''
            ]
            
            if self.sequence_mode == 'packet':
                # packet 模式：将每个包切分成子包
                max_tokens = self.maxlen - 2  # 减去 CLS 和 SEP
                
                for packet in packets:
                    # 编码包数据
                    packet_tokens = self.tokenizer.encode(packet)
                    
                    # 切分子包
                    subpackets = []
                    for i in range(0, len(packet_tokens), max_tokens):
                        subpacket = packet_tokens[i:i + max_tokens]
                        if len(subpacket) > 0:
                            subpackets.append(subpacket)
                    
                    # 添加到数据集
                    examples.extend(subpackets)
                    labels.extend([app_name] * len(subpackets))
            
            elif self.sequence_mode == 'flow':
                # flow 模式：保持完整的流结构
                flow_tokens = []
                for packet in packets:
                    packet_tokens = self.tokenizer.encode(packet)
                    flow_tokens.extend(packet_tokens)
                
                # 如果流太长，截断
                if len(flow_tokens) > self.maxlen - 2:
                    flow_tokens = flow_tokens[:self.maxlen - 2]
                
                examples.append(flow_tokens)
                labels.append(app_name)
        
        # 创建标签映射
        procedure_mapping = {name: idx for idx, name in enumerate(sorted(procedure_names))}
        
        # 转换标签为数字
        numeric_labels = [procedure_mapping[label] for label in labels]
        
        print(f"处理完成: {len(examples)} 个样本, {len(procedure_mapping)} 个应用类别")
        print(f"应用类别: {list(procedure_mapping.keys())}")
        
        return examples, numeric_labels, procedure_mapping
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        tokens = self.examples[idx]
        label = self.labels[idx]
        
        # 添加 CLS 和 SEP 标记
        tokens = [self.tokenizer.cls_token_id] + tokens + [self.tokenizer.sep_token_id]
        
        # 填充到固定长度
        if len(tokens) < self.maxlen:
            tokens += [self.tokenizer.pad_token_id] * (self.maxlen - len(tokens))
        else:
            tokens = tokens[:self.maxlen]
        
        return {
            'input_ids': torch.tensor(tokens, dtype=torch.long),
            'attention_mask': torch.tensor([1 if t != self.tokenizer.pad_token_id else 0 for t in tokens], dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class SimpleDataCollator:
    """简化的数据收集器，用于批处理"""
    
    def __init__(self, tokenizer, mask_percent: float = 0.15):
        self.tokenizer = tokenizer
        self.mask_percent = mask_percent
    
    def __call__(self, batch):
        # 收集批次数据
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])
        labels = torch.stack([item['labels'] for item in batch])
        
        # 创建 MLM 标签
        mlm_labels = input_ids.clone()
        
        # 随机掩码
        for i in range(input_ids.size(0)):
            for j in range(input_ids.size(1)):
                if (attention_mask[i, j] == 1 and 
                    input_ids[i, j] not in [self.tokenizer.cls_token_id, self.tokenizer.sep_token_id, self.tokenizer.pad_token_id] and
                    np.random.random() < self.mask_percent):
                    
                    # 80% 概率用 [MASK] 替换
                    if np.random.random() < 0.8:
                        input_ids[i, j] = self.tokenizer.mask_token_id
                    # 10% 概率用随机 token 替换
                    elif np.random.random() < 0.5:
                        input_ids[i, j] = np.random.randint(5, len(self.tokenizer))
                    # 10% 概率保持不变
                    else:
                        pass
                else:
                    mlm_labels[i, j] = -100  # 忽略这些位置
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'mlm_labels': mlm_labels
        }
```


```python
# 2. 初始化 tokenizer
tokenizer = SimpleTokenizer() # SimpleTokenizer Class
print(f"Tokenizer 词汇表大小: {len(tokenizer)}")
print(f"特殊标记: CLS={tokenizer.cls_token_id}, SEP={tokenizer.sep_token_id}, MASK={tokenizer.mask_token_id}, PAD={tokenizer.pad_token_id}, UNK={tokenizer.unk_token_id}\n")

# 3. 创建数据集
print("创建数据集...")
dataset = TrafficDataset(
    data_path=data_path,
    tokenizer=tokenizer,
    maxlen=256,  # 较小的序列长度用于演示
    sequence_mode='packet'  # 使用 packet 模式
)

# 4. 创建数据收集器
collator = SimpleDataCollator(tokenizer, mask_percent=0.15)

# 5. 创建数据加载器
dataloader = DataLoader(
    dataset,
    batch_size=2,
    shuffle=True,
    collate_fn=collator
)
```

    Tokenizer 词汇表大小: 261
    特殊标记: CLS=2, SEP=3, MASK=4, PAD=0, UNK=1
    
    创建数据集...
    加载数据: 2 条记录
    处理完成: 8 个样本, 1 个应用类别
    应用类别: ['cn.emagsoftware.gamehall']



```python


# 6. 演示数据处理
print("处理批次数据...")
for batch_idx, batch in enumerate(dataloader):
    print(f"\n--- 批次 {batch_idx + 1} ---")
    
    input_ids = batch['input_ids']
    attention_mask = batch['attention_mask']
    labels = batch['labels']
    mlm_labels = batch['mlm_labels']
    
    print(f"输入形状: {input_ids.shape}")
    print(f"注意力掩码形状: {attention_mask.shape}")
    print(f"应用标签: {labels.tolist()}")
    
    # 显示第一个样本的详细信息
    print(f"\n第一个样本:")
    print(f"输入 tokens (前20个): {input_ids[0][:20].tolist()}")
    print(f"注意力掩码 (前20个): {attention_mask[0][:20].tolist()}")
    print(f"MLM 标签 (前20个): {mlm_labels[0][:20].tolist()}")
    
    # 解码显示原始数据
    original_tokens = []
    for i, token_id in enumerate(input_ids[0]):
        if attention_mask[0][i] == 1 and token_id not in [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]:
            original_tokens.append(tokenizer.id_to_token[token_id.item()])
    
    print(f"原始十六进制数据 (前10个): {original_tokens[:10]}")
    
    # 只处理第一个批次用于演示
    break

print(f"\n=== 处理完成 ===")
print(f"总样本数: {len(dataset)}")
print(f"应用类别数: {len(dataset.procedure_mapping)}")
print(f"应用类别映射: {dataset.procedure_mapping}")
```

    处理批次数据...
    
    --- 批次 1 ---
    输入形状: torch.Size([2, 256])
    注意力掩码形状: torch.Size([2, 256])
    应用标签: [0, 0]
    
    第一个样本:
    输入 tokens (前20个): [2, 1, 4, 1, 17, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1]
    注意力掩码 (前20个): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    MLM 标签 (前20个): [-100, -100, 1, -100, 1, -100, -100, -100, -100, -100, 1, -100, -100, -100, -100, 1, -100, -100, -100, -100]
    原始十六进制数据 (前10个): ['[UNK]', '[MASK]', '[UNK]', '0c', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[MASK]']
    
    === 处理完成 ===
    总样本数: 8
    应用类别数: 1
    应用类别映射: {'cn.emagsoftware.gamehall': 0}



```python
def show_data_flow():
    """展示数据流向"""
    print("\n=== 数据流向说明 ===")
    print("1. CSV 文件读取")
    print("   - five_tuple: 五元组信息")
    print("   - app_name: 应用名称（用作标签）")
    print("   - flow_data: 十六进制流量数据")
    
    print("\n2. 流量数据解析")
    print("   - 按 '#' 分割得到多个包")
    print("   - 过滤掉 '0000' 空包")
    print("   - 将十六进制字符串编码为 token IDs")
    
    print("\n3. 子包切分 (packet 模式)")
    print("   - 将每个包切分成固定长度的子包")
    print("   - 添加 CLS 和 SEP 标记")
    print("   - 填充到固定长度")
    
    print("\n4. 掩码处理")
    print("   - 随机掩码 15% 的 token")
    print("   - 80% 用 [MASK] 替换")
    print("   - 10% 用随机 token 替换")
    print("   - 10% 保持不变")
    
    print("\n5. 批次处理")
    print("   - 收集多个样本组成批次")
    print("   - 创建注意力掩码")
    print("   - 准备 MLM 标签")
    
    print("\n6. 模型输入")
    print("   - input_ids: 输入 token 序列")
    print("   - attention_mask: 注意力掩码")
    print("   - mlm_labels: 掩码语言模型标签")
    print("   - labels: 应用分类标签")
```


```python
# 运行演示
show_data_flow()


```

    
    === 数据流向说明 ===
    1. CSV 文件读取
       - five_tuple: 五元组信息
       - app_name: 应用名称（用作标签）
       - flow_data: 十六进制流量数据
    
    2. 流量数据解析
       - 按 '#' 分割得到多个包
       - 过滤掉 '0000' 空包
       - 将十六进制字符串编码为 token IDs
    
    3. 子包切分 (packet 模式)
       - 将每个包切分成固定长度的子包
       - 添加 CLS 和 SEP 标记
       - 填充到固定长度
    
    4. 掩码处理
       - 随机掩码 15% 的 token
       - 80% 用 [MASK] 替换
       - 10% 用随机 token 替换
       - 10% 保持不变
    
    5. 批次处理
       - 收集多个样本组成批次
       - 创建注意力掩码
       - 准备 MLM 标签
    
    6. 模型输入
       - input_ids: 输入 token 序列
       - attention_mask: 注意力掩码
       - mlm_labels: 掩码语言模型标签
       - labels: 应用分类标签


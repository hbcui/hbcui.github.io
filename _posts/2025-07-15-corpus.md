# Traffic_Corpus_README_流量语料预处理的设计和实现文档

## 各阶段实现：Py

### 阶段1

代码只处理包含 "_filter" 的PCAP文件。

```
def create_run(path, save_path):
  ......
    for date_file_name in date_files_name:  # 扫描所有日期文件夹下的数据
        date_file_path = os.path.join(path, date_file_name)
        apps_name = os.listdir(date_file_path)

        # 每次处理一个文件夹下的数据
        app_pcaps_list = []

        for app_name in apps_name:  # 每个日期文件夹下的app
            app_path = os.path.join(date_file_path, app_name)
            apps_version = os.listdir(app_path)

            for app_version in apps_version:
                app_version_path = os.path.join(app_path, app_version)
                app_pcaps_name = os.listdir(app_version_path)
                app_pcaps_name = [file for file in app_pcaps_name if "_filter" in file] # 只处理包含 "_filter" 的PCAP文件
```

输出目录在下面。注意save_path_csv替换成你在shell脚本里面自己的路径。

```
save_path_csv/
├── 训练/
│   └── {datetime_str}/
│       └── training.csv
├── 验证/
│   └── {datetime_str}/
│       └── validation.csv
└── 评测/
    └── {datetime_str}/
        └── testing.csv
```

#### 包解析五元组协议

看这个`stage1_traffic_pcap_download_and_process.py`。修改。

```python
def pcap2csv_save(sessions, app_name, app_version, root_save_path):
    """ 将传入的sessions，保存为csv文件 """
    traffic_raw = []
    app_flow_nums = defaultdict(int)

    for key, data in sessions.items():
        str_data = '#'.join(data)
        packet_num = len(data)
        flow_len = len(str_data) // 2 - packet_num + 1
        # traffic_raw_single = [key, app_name, app_version, flow_len, packet_num, str_data]         # 原始代码保留
        # 新增：从五元组中提取协议信息   
        protocol = key.split('_')[-1] if '_' in key else 'UNKNOWN'
        traffic_raw_single = [key, app_name, app_version, protocol, flow_len, packet_num, str_data]
        traffic_raw.append(traffic_raw_single)

    # 原始代码保留
    # flow_wdf = pd.DataFrame(traffic_raw,
    #                         columns=['five_tuple', 'app_name', 'app_version', 'flow_len', 'packet_num', 'flow_data'])
    # 新增：包含协议字段的DataFrame
    flow_wdf = pd.DataFrame(traffic_raw,
                            columns=['five_tuple', 'app_name', 'app_version', 'protocol', 'flow_len', 'packet_num', 'flow_data'])
```

修改`processing/traffic_data_csv2csv.py`更新列名。

```python
            # 原始代码保留
            # processed_flow = [row['five_tuple'], row['app_name'], row['app_version'],
            #                   row['flow_len'], row['packet_num'], target_flow]
            
            # 新增：包含协议字段
            processed_flow = [row['five_tuple'], row['app_name'], row['app_version'],
                              row['protocol'], row['flow_len'], row['packet_num'], target_flow]
```

下面是产生的csv新增第四列：`protocol`。

```
five_tuple,app_name,app_version,protocol,flow_len,packet_num,flow_data
192.168.0.3:53265_211.152.118.11:80_TCP,WeChat,1.1.1,TCP,523,2,474554202f76332f6c6973743f6469643d62313033643635303166333162333666313934313634663935323563303631343362
```

#### 可选是否从IP头部开始的payload

增加payload_mode，下面是在`stage1_traffic_pcap_download_and_process.py`中的修改。

```python
def parse_pcap(app_pcap_list, save_path, payload_mode='payload'):
    ......................
                try:
                    response_data = parse_packet(buf, payload_mode=payload_mode)
                except Exception as e:

def create_run(path, save_path, payload_mode='payload'):
    ...............

if __name__ == "__main__":
    '''
    eg.
        hdfs_base_path = '/data1/scrawl/pcap'
        local_base_path = '/data1/traffic_data/pcap'
        target_base_path = '/data1/traffic_data/pcap_split'
        save_path_pcap = '/data1/traffic_data/filter_processed_pcap'
        save_path_csv = '/data1/traffic_data/filter_processed_csv'
    '''
    # 参数输入解析
    ...............................
    parser.add_argument("--payload_mode", type=str, default='payload', help="payload保存模式: payload(只保存payload) 或 with_header(保存带头部的payload)")
    args = parser.parse_args()

    ##### step 1: 流量数据下载
    .................
        create_run(target_base_path, save_path_pcap, args.payload_mode)
```

修改`processing/parse_pacp.py`支持两种模式。在`parse_packet`增加参数`payload_mode`并在调用时传递。

```python
# 新增：添加payload_mode参数
def parse_packet(packet, payload_mode='payload'):
  .................
    # 根据配置获取payload字节
    # save_data = ''
    # save_data = trans_layer.data.hex()
    if payload_mode == 'with_header':
        # 保存IP层及以上所有头部+数据
        save_data = ip.pack().hex()
    else:
        # 只保存payload
        save_data = trans_layer.data.hex()
```

最后修改一下控制流程的脚本文件`run.sh`。在最后一行增加` --payload_mode 'with_header'`。

```bash
case $SCRIPT_TYPE in
    "stage1")
        echo "Running stage1: traffic pcap download and process..."
        # If in MacOS, use python3; if in Linux, use python, if in Windows, use python.
        # 新增：use_tokenizer 'false'， 默认不使用tokenizer
        # 新增：添加payload_mode参数，默认保存payload
        python stage1_traffic_pcap_download_and_process.py \
        --hdfs_base_path '/data1/scrawl/pcap' \
        --local_base_path '/home/10354278/下载/pcap' \
        --target_base_path '/home/10354278/下载/data_processed/target' \
        --save_path_pcap '/home/10354278/下载/data_processed/pcap' \
        --save_path_csv '/home/10354278/下载/data_processed/csv' \
        --use_tokenizer 'false' \
        --payload_mode 'with_header'
        ;;
```

验证结果。运行`run.sh`第一阶段：

```bash
./run.sh stage1
```

首先打开Wireshark查看`com.taobao_1.1.7_2025.07.14.7.14_200_filter.pcap`前3行。

```
0000   f6 7b 09 3f be 25 52 5a d9 7c 53 45 08 00 45 00
0010   04 4c 11 ec 00 00 40 11 8c b2 dc ce 91 11 dc ce
0020   91 31 08 68 08 68 04 38 00 00 30 ff 04 28 63 00
...
```

发现Wireshark hex前14字节（f6 7b 09 3f be 25 52 5a d9 7c 53 45 08 00）是以太网头部。从第15字节开始看，45 00 04 4c..............

查看CSV数据, 没有以太网头部（没有f6 7b 09 3f ...），而是直接从45 00 ...开始。符合！！

```
220.206.145.17:2152_220.206.145.49:2152_UDP,com.taobao,7.14,UDP,62799179,77077,4500044c11ec000040118cb2................
```

#### 可选是否Tokenizer（默认不）

之前使用Tokenizer（Bigram），注意每次处理4个字符，但滑动窗口只是2个字符，也就是移动2个位置！！！输出每4个十六进制字符之间用空格分隔，所以会输出：

```
five_tuple,app_name,app_version,flow_len,packet_num,flow_data
220.206.145.17:2152_220.206.145.49:2152_UDP,com.taobao,7.14,60641023,77077,30ff ff04 0428 2863 6300 0000 0000 0045 4500 0004 0428 28f4 f468 6840 4000 0040 4006 068a 8ae2 e2c0 c0a8 a801 0116 1675 7587 87c2 c251 51b6 b694 9401 01bb bb5a 5a66 6601 0147 47d4 d4a2 a23c 3c46 4650 5010 ..........................
```

现在可以选择不使用Tokenizer，默认不使用了：

```bash
case $SCRIPT_TYPE in
    "stage1")
        .........
        --use_tokenizer 'false'
        ;;
```

在`./processing/traffic_data_csv2csv.py`中修改。

```python
class FlowProcessor_csv2csv:
    def __init__(self,
                 csv_data_dir,
                 processed_save_dir,
                 processed_save_file='processedFlows_Bigram_slide_Signaling.csv',
                 target_length=16 * 1024,
                 tokenizer_='Bigram'):
        ................
        self.tokenizer_ = tokenizer_
        self.save_raw_bytes = (tokenizer_ == 'raw_bytes') # 新增：是否保存原始字节流
        ................
    def create_dataset(self, data, type):   
            .....................
            # tk1 = tokenizer.Tokenizer(self.tokenizer_) # 在外面初始化
            # target_flow = tk1.tokenize(target_flow)
            # 根据配置决定是否进行token化
            if not self.save_raw_bytes:
                tk1 = tokenizer.Tokenizer(self.tokenizer_) # 在外面初始化
                target_flow = tk1.tokenize(target_flow)
```

修改`stage1_traffic_pcap_download_and_process.py`。也要修改`one_click.py`。

```python
    parser.add_argument("--use_tokenizer", type=str, default='true', help="是否使用tokenizer (true/false)")
    ................
    # tokenizer =  'Bigram' # 默认使用Bigram tokenizer，现在默认可以不使用tokenizer，在run.sh中设置 --use_tokenizer 'false'。
    # 对上一行的修改：根据参数决定是否使用tokenizer
    # 根据参数决定是否使用tokenizer
    if args.use_tokenizer.lower() == 'true':
        tokenizer_ = 'Bigram'
    else:
        tokenizer_ = 'raw_bytes'
```

现在解析出的flow_data原始字节流：

```
five_tuple,app_name,app_version,flow_len,packet_num,flow_data
220.206.145.17:2152_220.206.145.49:2152_UDP,com.taobao,7.14,60641023,77077,30ff04286300000045000428f468400040068ae2c0a801167587c25.....................
```

### 阶段2

修改`stage2_traffic_corpus_statistic_single_node.py`。更新统计对象。

```python
    # 新增：统计协议分布
    # 原始代码保留
    # 新增：协议统计
    if 'protocol' in df.columns:
        protocol_counts = df['protocol'].value_counts().to_dict()
        print(f"Protocol distribution for {app_name}: {protocol_counts}")
```